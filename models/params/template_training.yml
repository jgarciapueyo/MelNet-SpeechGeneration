# The training YAML file contains information needed to train the model:
# - information about the dataset and how to transform the audio to melspectrograms,
# - information about the architecture of the network (n_tiers, layers of each tier, ...),
# - information about the dataset (name, path to the dataset, ...)
# - information about the training (optimizer, epochs, ...)
# - information about logging the training

# [DESCRIPTION OF THE MODEL ARCHITECTURE: i.e. this model is a simple model to check that everything
# works]

# (str) identifies the model to be trained and its parameters
name: 'template_model'

# Parameters related with the datasets pipeline (transforms between different audio representations like
# audio waveform, spectrogram, melspectrogram, ...).
audio:
  # (int): sample rate of the sample audio in Hz
  sample_rate: 16000
  # (str) type of spectrogram it will represent: 'linear' for energy, 'power' for power
  spectrogram_type: 'power'
  # (int) size of the Fast Fourier Transform
  n_fft: 1536  # 6 * 256
  # (int) number of mel channels when transforming to MelSpectrogram
  mel_channels: 256
  # (int) length of hop between STFT windows
  hop_length: 256
  # (int) window size
  win_length: 1536  # 6 * 256
  # (int) reference level (in dB) in spectrogram
  ref_level_db: 20
  # (int) minimum level (in dB) in spectrogram
  min_level_db: -100

# Parameters related with the structure of the network (n_tiers, layers of each tier, ...).
network:
  # (int)
  n_tiers: 6
  # (List[int])
  layers: [12,5,4,3,2,2]
  # (int)
  hidden_size: 512
  # (int)
  gmm_size: 10

# Information related to the folder where the datasets for training is found and additional information
# depending on the dataset being used.
# NOTE: The fields required change depending on the dataset being used
data:
  dataset: 'librispeech'
  path: "datasets/librispeech/"
  url: "dev-clean"

# Parameters related with the training phase (optimizer, learning rate, ...)
training:
  # (str)
  optimizer: 'RMSProp'
  # (int)
  epochs: 2
  # (int)
  batch_size: 1
  # (float)
  lr: 0.0010
  # (float)
  momentum: 0.9
  # (int) number of workers for loading the datasets
  num_workers: 1
  # (int) iterations between model save
  save_iterations: 40
  # (str) path where to save model weights
  dir_chkpt: 'models/chkpt/'

# Information related to the folder and paths where to save the logs and the model weights during
# training
logging:
  # (str) path where to save the tensorboard logs
  dir_log_tensorboard: 'logs/tensorboard/'
  # (str) path where to save other logs
  dir_log: 'logs/general/'
  # (int) iterations between logs
  log_iterations: 1
